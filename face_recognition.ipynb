{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e8c55d-08c8-483a-9203-e4ec094ced31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch import nn \n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49fa1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "512ad5f3-5df0-4a73-8385-72e4266fdfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\moaaz/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "# setup training device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# get efficientnet-b0 as a backbone\n",
    "backbone = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "\n",
    "# delete the last classifier layer\n",
    "backbone.classifier.fc=nn.Identity() \n",
    "backbone.classifier.dropout=nn.Identity()\n",
    "\n",
    "# freeze all parameters for efficientnet-b0\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# putting model on device \n",
    "efficientnet= backbone.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "941c3da3-98a7-40bb-bbff-3c3d0d24b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese network\n",
    "class Model(nn.Module): \n",
    "    def __init__(self  ): \n",
    "        \"\"\" \n",
    "        model input shape is (batch size , CH, W , H) \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.Sequential( \n",
    "            nn.AvgPool1d(kernel_size = 3, stride=2 , padding=1)\n",
    "        )\n",
    "        self.f1 = nn.Sequential( \n",
    "            nn.Linear(in_features=640, out_features=1024 ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2 ),\n",
    "            nn.BatchNorm1d(num_features=1024)\n",
    "        ) \n",
    "        self.f2 = nn.Sequential( \n",
    "            nn.Linear(in_features=1024, out_features=512 ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2 ),\n",
    "            nn.BatchNorm1d(num_features=512)\n",
    "        )\n",
    "        self.f3= nn.Sequential( \n",
    "            nn.Linear(in_features=512, out_features=256 ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2 ),\n",
    "            nn.BatchNorm1d(num_features=256))\n",
    "        self.embed= nn.Sequential( \n",
    "            nn.Linear(in_features=256, out_features=128 ),\n",
    "        ) \n",
    "    def forward (self , x ):\n",
    "        x= backbone(x)\n",
    "        x=self.pool(x)\n",
    "        x= self.f1(x)\n",
    "        x= self.f2(x)\n",
    "        x= self.f3(x)\n",
    "        x= self.embed(x)\n",
    "        return x\n",
    "        \n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d99050a2-efa3-4e6f-9cc8-9519829e366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our loss calculation class \n",
    "class loss_fn(): \n",
    "    def __init__(self, margin , model):\n",
    "        \n",
    "        self.margin = torch.tensor(margin).to(device)\n",
    "        self.model = model\n",
    "        \n",
    "    def _embedding(self,inputs : list[torch.tensor]):\n",
    "        \"\"\"\n",
    "        inputs : list[torch.tensor] with the shape (positive, anchor, negative)\n",
    "        output : list[torch.tensor] with the shape (positive, anchor, negative)\n",
    "        \"\"\"\n",
    "        # [3,3,256,256]\n",
    "        positive = self.model(inputs[0])\n",
    "        anchor = self.model(inputs[1])\n",
    "        negative = self.model(inputs[2])\n",
    "\n",
    "        return [positive,anchor,negative]\n",
    "        \n",
    "    def compute_distance(self, inputs):\n",
    "\n",
    "        embeddings = self._embedding(inputs)\n",
    "        \n",
    "        anchorEmbedding = embeddings[1]\n",
    "        positiveEmbedding = embeddings[0]\n",
    "        negativeEmbedding = embeddings[2]\n",
    "    \n",
    "        # calculate the anchor to positive and negative distance\n",
    "        apDistance = torch.sum( torch.square(anchorEmbedding - positiveEmbedding), axis=-1)\n",
    "        anDistance = torch.sum( torch.square(anchorEmbedding - negativeEmbedding), axis=-1 )\n",
    "        return apDistance , anDistance\n",
    "    \n",
    "    def compute_loss(self , apDistance , anDistance):\n",
    "        # return the distances\n",
    "        loss = apDistance - anDistance\n",
    "        loss = torch.max(loss + self.margin, torch.tensor(0.0).to(device))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19f165ec-8b08-499a-9704-71fba6cfdd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8a76632-2adc-4d9e-afc0-ca0b397df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = loss_fn(0.5, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c234f",
   "metadata": {},
   "source": [
    "# Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f55faffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_data_loader(datasets.ImageFolder):\n",
    "    def __init__(self, *arg, **kw) :\n",
    "        super(custom_data_loader, self).__init__(*arg, **kw)\n",
    "        self.n_triplets =len(self.samples)\n",
    "        \n",
    "        self.train_triplets = self.gen_example()\n",
    "    def __len__(self):\n",
    "        return (len(self.targets))\n",
    "\n",
    "    def gen_example(self ): \n",
    "       \n",
    "       labels = torch.Tensor(self.targets)\n",
    "    \n",
    "       triplets = []\n",
    "       for x in np.arange(self.n_triplets): \n",
    "            \n",
    "            idx = np.random.randint(0, labels.size(0))\n",
    "            idx_matches = np.where(labels.numpy() == labels[idx].numpy())[0] \n",
    "            idx_no_matches = np.where(labels.numpy() != labels[idx].numpy())[0]\n",
    "            idx_a, idx_p = np.random.choice(idx_matches, 2, replace=True)\n",
    "            idx_n = np.random.choice(idx_no_matches, 1)[0]\n",
    "            triplets.append([  idx_a,idx_p, idx_n])\n",
    "       return np.array(triplets)\n",
    "    \n",
    "    def set_triplets(self, triplets):\n",
    "        self.train_triplets = triplets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.train_triplets[idx]\n",
    "\n",
    "        path_a, _ = self.samples[t[0]]\n",
    "        path_p, _ = self.samples[t[1]]\n",
    "        path_n, _ = self.samples[t[2]]\n",
    "\n",
    "        img_a = self.loader(path_a)\n",
    "        img_p = self.loader(path_p)\n",
    "        img_n = self.loader(path_n)\n",
    "        if self.transform is not None:\n",
    "            img_a = self.transform(img_a)\n",
    "            img_p = self.transform(img_p)\n",
    "            img_n = self.transform(img_n)\n",
    "        \n",
    "        return img_p , img_a , img_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63bed995",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform= data_transform = transforms.Compose([\n",
    "            transforms.Resize(size=config.IMAGE_SIZE ),\n",
    "            transforms.ToTensor() \n",
    "        ])\n",
    "triplet_data_train  =custom_data_loader( root=config.TRAIN_DATASET, transform=data_transform  )\n",
    "triplet_dataloader_train  = DataLoader(triplet_data_train, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea3fcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_data_test  =custom_data_loader( root=config.TEST_DATASET, transform=data_transform  )\n",
    "triplet_dataloader_test  = DataLoader(triplet_data_test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d39935b2-2ccd-4cbb-865e-588ec2740184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               train_data ,\n",
    "               test_data,\n",
    "               loss_fn,\n",
    "               optimizer,\n",
    "               device: torch.device = device ,\n",
    "               epochs : int = 100):\n",
    "    train_loss_acc = []\n",
    "    test_loss_acc= []\n",
    "    for i in range(epochs):\n",
    "       \n",
    "        train_loss, train_acc = 0, 0\n",
    "        test_loss, test_acc =0,0\n",
    "        for batch, (p , a,n ) in enumerate(train_data):\n",
    "        # Send data to GPU\n",
    "        # postive , anchor , negative = postive.to(device), anchor.to(device), negative.to(device)\n",
    "            p ,a ,n = p.to(device),a.to(device),n.to(device)\n",
    "            inputs = torch.stack([p,a,n] , dim =0)\n",
    "        # 2. Calculate loss\n",
    "            \n",
    "            apDistance , anDistance = loss_fn.compute_distance(inputs)\n",
    "            loss = loss_fn.compute_loss(apDistance , anDistance  )\n",
    "\n",
    "            train_loss += loss.sum()\n",
    "        # 3. Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "        # 4. Loss backward\n",
    "            loss.sum().backward()\n",
    "        # 5. Optimizer step\n",
    "            optimizer.step() \n",
    "        train_loss_acc.append(train_loss.detach().to(\"cpu\").numpy())\n",
    "        model.eval()\n",
    "        with torch.no_grad() : \n",
    "            for batch, (p , a,n ) in enumerate(test_data):\n",
    "        \n",
    "        # Send data to GPU\n",
    "        # postive , anchor , negative = postive.to(device), anchor.to(device), negative.to(device)\n",
    "                p ,a ,n = p.to(device),a.to(device),n.to(device)\n",
    "                inputs = torch.stack([p,a,n] , dim =0)\n",
    "        # 2. Calculate loss\n",
    "            \n",
    "                apDistance , anDistance = loss_fn.compute_distance(inputs)\n",
    "                loss = loss_fn.compute_loss(apDistance , anDistance  )\n",
    " \n",
    "                test_loss += loss.sum()\n",
    "\n",
    "            test_loss_acc.append(test_loss.to(\"cpu\").detach().numpy())\n",
    "        print(f\"train loss: {train_loss} test loss: {test_loss} @ epoch {i}\")\n",
    "    return train_loss_acc , test_loss_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a779c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 45.97726058959961 test loss: 4.542803764343262 @ epoch 0\n",
      "train loss: 6.645621299743652 test loss: 4.683127403259277 @ epoch 1\n",
      "train loss: 5.3045735359191895 test loss: 4.951138496398926 @ epoch 2\n",
      "train loss: 3.935481548309326 test loss: 5.320071220397949 @ epoch 3\n",
      "train loss: 2.4137771129608154 test loss: 5.638318061828613 @ epoch 4\n",
      "train loss: 1.5338997840881348 test loss: 5.8872809410095215 @ epoch 5\n",
      "train loss: 1.041686773300171 test loss: 6.031423091888428 @ epoch 6\n",
      "train loss: 0.6912530660629272 test loss: 6.104761123657227 @ epoch 7\n",
      "train loss: 0.16672658920288086 test loss: 6.15138053894043 @ epoch 8\n",
      "train loss: 0.0 test loss: 6.180362701416016 @ epoch 9\n",
      "train loss: 0.0 test loss: 6.200336933135986 @ epoch 10\n",
      "train loss: 0.0 test loss: 6.2141432762146 @ epoch 11\n",
      "train loss: 0.0 test loss: 6.225149154663086 @ epoch 12\n",
      "train loss: 0.0 test loss: 6.232798099517822 @ epoch 13\n",
      "train loss: 0.0 test loss: 6.2378973960876465 @ epoch 14\n",
      "train loss: 0.0 test loss: 6.241369247436523 @ epoch 15\n",
      "train loss: 0.0 test loss: 6.243750095367432 @ epoch 16\n",
      "train loss: 0.0 test loss: 6.245340824127197 @ epoch 17\n",
      "train loss: 0.0 test loss: 6.246417045593262 @ epoch 18\n",
      "train loss: 0.0 test loss: 6.247185707092285 @ epoch 19\n",
      "train loss: 0.0 test loss: 6.247702121734619 @ epoch 20\n",
      "train loss: 0.0 test loss: 6.248046398162842 @ epoch 21\n",
      "train loss: 0.0 test loss: 6.248297214508057 @ epoch 22\n",
      "train loss: 0.0 test loss: 6.248466968536377 @ epoch 23\n",
      "train loss: 0.0 test loss: 6.248579025268555 @ epoch 24\n",
      "train loss: 0.0 test loss: 6.248655319213867 @ epoch 25\n",
      "train loss: 0.0 test loss: 6.24870491027832 @ epoch 26\n",
      "train loss: 0.0 test loss: 6.248739242553711 @ epoch 27\n",
      "train loss: 0.0 test loss: 6.2487616539001465 @ epoch 28\n",
      "train loss: 0.0 test loss: 6.248775959014893 @ epoch 29\n",
      "train loss: 0.0 test loss: 6.248785018920898 @ epoch 30\n",
      "train loss: 0.0 test loss: 6.248791217803955 @ epoch 31\n",
      "train loss: 0.0 test loss: 6.248795509338379 @ epoch 32\n",
      "train loss: 0.0 test loss: 6.248798370361328 @ epoch 33\n",
      "train loss: 0.0 test loss: 6.2487993240356445 @ epoch 34\n",
      "train loss: 0.0 test loss: 6.248800277709961 @ epoch 35\n",
      "train loss: 0.0 test loss: 6.248799800872803 @ epoch 36\n",
      "train loss: 0.0 test loss: 6.248800277709961 @ epoch 37\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 38\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 39\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 40\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 41\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 42\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 43\n",
      "train loss: 0.0 test loss: 6.248800277709961 @ epoch 44\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 45\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 46\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 47\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 48\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 49\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 50\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 51\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 52\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 53\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 54\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 55\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 56\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 57\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 58\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 59\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 60\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 61\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 62\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 63\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 64\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 65\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 66\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 67\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 68\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 69\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 70\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 71\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 72\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 73\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 74\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 75\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 76\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 77\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 78\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 79\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 80\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 81\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 82\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 83\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 84\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 85\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 86\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 87\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 88\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 89\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 90\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 91\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 92\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 93\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 94\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 95\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 96\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 97\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 98\n",
      "train loss: 0.0 test loss: 6.248800754547119 @ epoch 99\n"
     ]
    }
   ],
   "source": [
    "train_loss , test_loss = train_step(model, triplet_dataloader_train , triplet_dataloader_test, loss_function, optimizer,  device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a5c51",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- get a better model\n",
    "- evaluation metrics\n",
    "- setup testing on a single input  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e13b86c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2234,  0.1222, -0.1239, -0.1642, -0.2062,  0.0816,  0.1660, -0.1628,\n",
       "          0.1041,  0.0876, -0.1518, -0.2030,  0.0851,  0.2966,  0.1166,  0.0952,\n",
       "         -0.1162,  0.0172, -0.1691,  0.2308,  0.0208,  0.2810, -0.2433,  0.0848,\n",
       "         -0.0793,  0.0678, -0.0411, -0.1001, -0.0844, -0.1642, -0.0314,  0.1733,\n",
       "          0.0057,  0.2140,  0.2378, -0.0044, -0.0849,  0.1237,  0.0316,  0.0678,\n",
       "          0.1436, -0.0590,  0.0117,  0.2900, -0.0457,  0.1262, -0.1463,  0.1422,\n",
       "          0.0050,  0.0482, -0.1706,  0.1340, -0.0666,  0.1389, -0.1058, -0.0157,\n",
       "          0.0182, -0.2733, -0.0520,  0.1310, -0.2148, -0.0417, -0.1726, -0.0980,\n",
       "          0.1629,  0.0353,  0.0504,  0.1300, -0.0464,  0.1694, -0.0888,  0.0077,\n",
       "          0.0493, -0.1013, -0.0331,  0.0960,  0.2619,  0.0717,  0.0127, -0.0680,\n",
       "          0.0646, -0.1401, -0.0491, -0.3085, -0.1358, -0.1194, -0.0946,  0.1434,\n",
       "          0.1243, -0.2062,  0.1443,  0.1248, -0.1398, -0.1277, -0.1244,  0.1654,\n",
       "         -0.1781,  0.0957,  0.1331, -0.0062, -0.1238,  0.0411, -0.1353, -0.1563,\n",
       "          0.1665,  0.1659,  0.0930, -0.0501,  0.1397, -0.0057,  0.0678, -0.0744,\n",
       "          0.0760,  0.2871, -0.1943,  0.1029,  0.0028,  0.2031,  0.1387,  0.1787,\n",
       "         -0.1564,  0.0188, -0.0743,  0.1887, -0.0163, -0.3629, -0.0738, -0.0008]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data[0][0].to(device).unsqueeze(0))\n",
    "# .to ( device ) => send data to the device we are using \n",
    "# unsqueeze(0) adds another dimension in the 0 axis to be able to pass it to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328623f7",
   "metadata": {},
   "source": [
    "give input of an image to the model => model -> embeddings ( vectors )\n",
    "to find the person we want we measure distance between this embedding and the embeddings of out dataset\n",
    "input - dataset \n",
    "\n",
    "open each folder \n",
    "make dictionary of the name of the person (folder name) as keys and values are the images\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
